{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "    { \"id\": 0, \"name\": \"Hero\" },\n",
    "    { \"id\": 1, \"name\": \"Dunn\" },\n",
    "    { \"id\": 2, \"name\": \"Sue\" },\n",
    "    { \"id\": 3, \"name\": \"Chi\" },\n",
    "    { \"id\": 4, \"name\": \"Thor\" },\n",
    "    { \"id\": 5, \"name\": \"Clive\" },\n",
    "    { \"id\": 6, \"name\": \"Hicks\" },\n",
    "    { \"id\": 7, \"name\": \"Devin\" },\n",
    "    { \"id\": 8, \"name\": \"Kate\" },\n",
    "    { \"id\": 9, \"name\": \"Klein\" }\n",
    "]\n",
    "\n",
    "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
    "               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “We also added friend lists to each user dict:\n",
    "for user in users:\n",
    "    user[\"friends\"] = []\n",
    "\n",
    "for i, j in friendships:\n",
    "    # this works because users[i] is the user whose id is i\n",
    "    users[i][\"friends\"].append(users[j]) # add i as a friend of j\n",
    "    users[j][\"friends\"].append(users[i]) # add j as a friend of i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def shortest_paths_from(from_user):\n",
    "\n",
    "    # a dictionary from \"user_id\" to *all* shortest paths to that user\n",
    "    shortest_paths_to = { from_user[\"id\"] : [[]] }\n",
    "\n",
    "    # a queue of (previous user, next user) that we need to check.\n",
    "    # starts out with all pairs (from_user, friend_of_from_user)\n",
    "    frontier = deque((from_user, friend)\n",
    "                     for friend in from_user[\"friends\"])\n",
    "\n",
    "    # keep going until we empty the queue\n",
    "    while frontier:\n",
    "        prev_user, user = frontier.popleft()   # remove the user who's\n",
    "        user_id = user[\"id\"]                   # first in the queue\n",
    "\n",
    "        # because of the way we're adding to the queue,\n",
    "        # necessarily we already know some shortest paths to prev_user\n",
    "        paths_to_prev_user = shortest_paths_to[prev_user[\"id\"]]\n",
    "        new_paths_to_user = [path + [user_id] for path in paths_to_prev_user]\n",
    "\n",
    "        # it's possible we already know a shortest path\n",
    "        old_paths_to_user = shortest_paths_to.get(user_id, [])\n",
    "\n",
    "        # what's the shortest path to here that we've seen so far?\n",
    "        if old_paths_to_user:\n",
    "            min_path_length = len(old_paths_to_user[0])\n",
    "        else:\n",
    "            min_path_length = float('inf')\n",
    "\n",
    "        # only keep paths that aren't too long and are actually new\n",
    "        new_paths_to_user = [path\n",
    "                             for path in new_paths_to_user\n",
    "                             if len(path) <= min_path_length\n",
    "                             and path not in old_paths_to_user]\n",
    "\n",
    "        shortest_paths_to[user_id] = old_paths_to_user + new_paths_to_user\n",
    "\n",
    "        # add never-seen neighbors to the frontier\n",
    "        frontier.extend((user, friend)\n",
    "                        for friend in user[\"friends\"]\n",
    "                        if friend[\"id\"] not in shortest_paths_to)\n",
    "\n",
    "    return shortest_paths_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Now we can store these dicts with each node:\n",
    "\n",
    "for user in users:\n",
    "    user[\"shortest_paths\"] = shortest_paths_from(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we’re finally ready to compute betweenness centrality. For every pair of nodes i and j, we know the n \n",
    "# shortest paths from i to j. Then, for each of those paths, we just add 1/n to the centrality of each node \n",
    "# on that path:\n",
    "\n",
    "for user in users:\n",
    "    user[\"betweenness_centrality\"] = 0.0\n",
    "\n",
    "for source in users:\n",
    "    source_id = source[\"id\"]\n",
    "    for target_id, paths in source[\"shortest_paths\"].iteritems():\n",
    "        if source_id < target_id:      # don't double count\n",
    "            num_paths = len(paths)     # how many shortest paths?\n",
    "            contrib = 1 / num_paths    # contribution to centrality\n",
    "            for path in paths:\n",
    "                for id in path:\n",
    "                    if id not in [source_id, target_id]:\n",
    "                        users[id][\"betweenness_centrality\"] += contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another measure we can look at is closeness centrality. First, for each user we compute her farness, \n",
    "# which is the sum of the lengths of her shortest paths to each other user. Since we’ve already \n",
    "# computed the shortest paths between each pair of nodes, it’s easy to add their lengths.\n",
    "\n",
    "def farness(user):\n",
    "    \"\"\"the sum of the lengths of the shortest paths to each other user\"\"\"\n",
    "    return sum(len(paths[0])\n",
    "               for paths in user[\"shortest_paths\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after which it’s very little work to compute closeness centrality\n",
    "\n",
    "for user in users:\n",
    "    user[\"closeness_centrality\"] = 1 / farness(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvector Centrality\n",
    "# In order to talk about eigenvector centrality, we have to talk about eigenvectors, and in order to talk about \n",
    "# eigenvectors, we have to talk about matrix multiplication.\n",
    "\n",
    "def matrix_product_entry(A, B, i, j):\n",
    "    return dot(get_row(A, i), get_column(B, j))\n",
    "\n",
    "def matrix_multiply(A, B):\n",
    "    n1, k1 = shape(A)\n",
    "    n2, k2 = shape(B)\n",
    "    if k1 != n2:\n",
    "        raise ArithmeticError(\"incompatible shapes!\")\n",
    "\n",
    "    return make_matrix(n1, k2, partial(matrix_product_entry, A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll need some helper functions to convert back and forth between the two representations:\n",
    "\n",
    "def vector_as_matrix(v):\n",
    "    \"\"\"returns the vector v (represented as a list) as a n x 1 matrix\"\"\"\n",
    "    return [[v_i] for v_i in v]\n",
    "\n",
    "def vector_from_matrix(v_as_matrix):\n",
    "    \"\"\"returns the n x 1 matrix as a list of values\"\"\"\n",
    "    return [row[0] for row in v_as_matrix]\n",
    "\n",
    "# after which we can define the matrix operation using matrix_multiply:\n",
    "\n",
    "def matrix_operate(A, v):\n",
    "    v_as_matrix = vector_as_matrix(v)\n",
    "    product = matrix_multiply(A, v_as_matrix)\n",
    "    return vector_from_matrix(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eigenvector(A, tolerance=0.00001):\n",
    "    guess = [random.random() for __ in A]\n",
    "\n",
    "    while True:\n",
    "        result = matrix_operate(A, guess)\n",
    "        length = magnitude(result)\n",
    "        next_guess = scalar_multiply(1/length, result)\n",
    "\n",
    "        if distance(guess, next_guess) < tolerance:\n",
    "            return next_guess, length   # eigenvector, eigenvalue\n",
    "\n",
    "        guess = next_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality\n",
    "# We’ll need to represent the connections in our network as an adjacency_matrix, whose (i,j)th entry is either 1\n",
    "# (if user i and user j are friends) or 0 (if they’re not):\n",
    "\n",
    "def make_matrix(num_rows, num_cols, entry_fn):\n",
    "    \"\"\"returns a num_rows x num_cols matrix\n",
    "    whose (i,j)th entry is entry_fn(i, j)\"\"\"\n",
    "    return [[entry_fn(i, j)             # given i, create a list\n",
    "             for j in range(num_cols)]  #   [entry_fn(i, 0), ... ]\n",
    "            for i in range(num_rows)]   # create one list for each i\n",
    "\n",
    "def entry_fn(i, j):\n",
    "    return 1 if (i, j) in friendships or (j, i) in friendships else 0\n",
    "\n",
    "n = len(users)\n",
    "adjacency_matrix = make_matrix(n, n, entry_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eigenvector centrality for each user is then the entry corresponding to that user in the eigenvector \n",
    "# returned by find_eigenvector\n",
    "\n",
    "\n",
    "# eigenvector_centralities, _ = find_eigenvector(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "endorsements = [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2),\n",
    "                (2, 1), (1, 3), (2, 3), (3, 4), (5, 4),\n",
    "                (5, 6), (7, 5), (6, 8), (8, 7), (8, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directed Graphs and PageRank\n",
    "\n",
    "for user in users:\n",
    "    user[\"endorses\"] = []       # add one list to track outgoing endorsements\n",
    "    user[\"endorsed_by\"] = []    # and another to track endorsements\n",
    "\n",
    "for source_id, target_id in endorsements:\n",
    "    users[source_id][\"endorses\"].append(users[target_id])\n",
    "    users[target_id][\"endorsed_by\"].append(users[source_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (3, 2),\n",
       " (4, 2),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endorsements_by_id = [(user[\"id\"], len(user[\"endorsed_by\"]))\n",
    "                      for user in users]\n",
    "\n",
    "sorted(endorsements_by_id,\n",
    "       key=lambda (user_id, num_endorsements): num_endorsements,\n",
    "       reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank(users, damping = 0.85, num_iters = 100):\n",
    "\n",
    "    # initially distribute PageRank evenly\n",
    "    num_users = len(users)\n",
    "    pr = { user[\"id\"] : 1 / num_users for user in users }\n",
    "\n",
    "    # this is the small fraction of PageRank\n",
    "    # that each node gets each iteration\n",
    "    base_pr = (1 - damping) / num_users\n",
    "\n",
    "    for __ in range(num_iters):\n",
    "        next_pr = { user[\"id\"] : base_pr for user in users }\n",
    "        for user in users:\n",
    "            # distribute PageRank to outgoing links\n",
    "            links_pr = pr[user[\"id\"]] * damping\n",
    "            for endorsee in user[\"endorses\"]:\n",
    "                next_pr[endorsee[\"id\"]] += links_pr / len(user[\"endorses\"])\n",
    "\n",
    "        pr = next_pr\n",
    "\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
