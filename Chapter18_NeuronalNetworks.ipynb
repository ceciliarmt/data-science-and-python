{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERCEPTRONS\n",
    "# the simplest neural network is the perceptron, which approximates a single neuron with n binary inputs. \n",
    "# It computes a weighted sum of its inputs and “fires” if that weighted sum is zero or greater\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def perceptron_output(weights, bias, x):\n",
    "    \"\"\"returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The perceptron is simply distinguishing between the half spaces separated by the hyperplane of points x for which:\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i\n",
    "        for v_i, w_i in zip(v, w))\n",
    "\n",
    "#dot(weights,x) + bias == 0\n",
    "#weights = [2, 2]\n",
    "#bias = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, there are some problems that simply can’t be solved by a single perceptron. For example, no matter how \n",
    "# hard you try, you cannot use a perceptron to build an XOR gate that outputs 1 if exactly one of its inputs is 1 \n",
    "# and 0 otherwise. This is where we start needing more-complicated neural networks.\n",
    "\n",
    "and_gate = min\n",
    "or_gate = max\n",
    "xor_gate = lambda x, y: 0 if x == y else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feed-forward neural network that consists of discrete layers of neurons, each connected to the next. \n",
    "# This typically entails an input layer, one or more “hidden layers (each of which consists of neurons that take \n",
    "# the outputs of the previous layer, performs some calculation, and passes the result to the next layer\n",
    "# and an output layer (which produces the final outputs)\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network\n",
    "    (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    # process one layer at a time\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias)  # compute the output\n",
    "                  for neuron in layer]                    # for each neuron\n",
    "        outputs.append(output)                            # and remember it\n",
    "\n",
    "        # then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 [9.38314668300676e-14]\n",
      "0 1 [0.9999999999999059]\n",
      "1 0 [0.9999999999999059]\n",
      "1 1 [9.383146683006828e-14]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "xor_network = [# hidden layer\n",
    "               [[20, 20, -30],      # 'and' neuron\n",
    "                [20, 20, -10]],     # 'or'  neuron\n",
    "               # output layer\n",
    "               [[-60, 60, -30]]]    # '2nd input but not 1st input' neuron\n",
    "\n",
    "for x in [0, 1]:\n",
    "    for y in [0, 1]:\n",
    "        # feed_forward produces the outputs of every neuron\n",
    "        # feed_forward[-1] is the outputs of the output-layer neurons\n",
    "        print x, y, feed_forward(xor_network,[x, y])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, targets):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, targets)]\n",
    "\n",
    "    # adjust weights for output layer, one neuron at a time\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        # focus on the ith output layer neuron\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # adjust the jth weight based on both\n",
    "            # this neuron's delta and its jth input\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                      dot(output_deltas, [n[i] for n in output_layer])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # adjust weights for hidden layer, one neuron at a time\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Defeating a CAPTCHA\n",
    "\n",
    "# In particular, he’d like to show users a picture of a digit and require them to input that digit to prove \n",
    "# they’re human.\n",
    "\n",
    "zero_digit = [1,1,1,1,1,\n",
    "              1,0,0,0,1,\n",
    "              1,0,0,0,1,\n",
    "              1,0,0,0,1,\n",
    "              1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We’ll want our output to indicate which digit the neural network thinks it is, so we’ll need 10 outputs. \n",
    "# The correct output for digit 4, for instance, will be:\n",
    "\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Then, assuming our inputs are correctly ordered from 0 to 9, our targets will be:\n",
    "\n",
    "targets = [[1 if i == j else 0 for i in range(10)]\n",
    "           for j in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At which point we’re ready to build our neural network:\n",
    "\n",
    "import random \n",
    "\n",
    "random.seed(0)      # to get repeatable results\n",
    "input_size = 25     # each input is a vector of length 25\n",
    "num_hidden = 5      # we'll have 5 neurons in the hidden layer\n",
    "output_size = 10    # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)]\n",
    "                for __ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)]\n",
    "                for __ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    raw_digits = [\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             1...1\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"..1..\n",
    "             ..1..\n",
    "             ..1..\n",
    "             ..1..\n",
    "             ..1..\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             11111\n",
    "             1....\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"1...1\n",
    "             1...1\n",
    "             11111\n",
    "             ....1\n",
    "             ....1\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1....\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1....\n",
    "             11111\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             ....1\n",
    "             ....1\n",
    "             ....1\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             11111\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\"]\n",
    "\n",
    "    def make_digit(raw_digit):\n",
    "        return [1 if c == '1' else 0\n",
    "                for row in raw_digit.split(\"\\n\")\n",
    "                for c in row.strip()]\n",
    "    \n",
    "inputs = list(map(make_digit, raw_digits))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can train it using the backpropagation algorithm:\n",
    "\n",
    "# 10,000 iterations seems enough to converge\n",
    "for __ in range(10000):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0253613342567603,\n",
       " 1.0763103307031964e-05,\n",
       " 1.2345860565941365e-10,\n",
       " 0.017928582780863102,\n",
       " 0.0008665440350657816,\n",
       " 5.979309218296072e-10,\n",
       " 3.267997415221431e-08,\n",
       " 0.967411806405591,\n",
       " 1.66229065217321e-08,\n",
       " 4.394241877748721e-08]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(input):\n",
    "    return feed_forward(network, input)[-1]\n",
    "\n",
    "predict(inputs[7])\n",
    "# [0.026, 0.0, 0.0, 0.018, 0.001, 0.0, 0.0, 0.967, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0101045598243533e-08,\n",
       " 0.001726887935497551,\n",
       " 1.78191113310607e-08,\n",
       " 0.9344276025778887,\n",
       " 5.097944343977291e-07,\n",
       " 3.6294250408123e-06,\n",
       " 2.2726529247374686e-10,\n",
       " 0.007047567870016199,\n",
       " 3.9216678958134186e-08,\n",
       " 0.09973408445781098]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which indicates that the digit 7 output neuron produces 0.97, while all the other output neurons \n",
    "# produce very small numbers. But we can also apply it to differently drawn digits, like my stylized 3:\n",
    "\n",
    "predict([0,1,1,1,0,  # .@@@.\n",
    "         0,0,0,1,1,  # ...@@\n",
    "         0,0,1,1,0,  # ..@@.\n",
    "         0,0,0,1,1,  # ...@@\n",
    "         0,1,1,1,0]) # .@@@.\n",
    "\n",
    "# [0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.01, 0.0, 0.12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0462000883297418e-06,\n",
       " 4.3282683762421793e-13,\n",
       " 7.538605319160643e-09,\n",
       " 0.00046289833754861753,\n",
       " 1.0265125035042659e-10,\n",
       " 0.5362247897883677,\n",
       " 2.63374015575664e-05,\n",
       " 6.728904926488385e-08,\n",
       " 0.9146090904428066,\n",
       " 0.9957966984536238]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The network still thinks it looks like a 3, whereas my stylized 8 gets votes for being a 5, an 8, and a 9:\n",
    "\n",
    "predict([0,1,1,1,0,  # .@@@.\n",
    "         1,0,0,1,1,  # @..@@\n",
    "         0,1,1,1,0,  # .@@@.\n",
    "         1,0,0,1,1,  # @..@@\n",
    "         0,1,1,1,0]) # .@@@.\n",
    "\n",
    "# [0.0, 0.0, 0.0, 0.0, 0.0, 0.55, 0.0, 0.0, 0.93, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "weights = network[0][0]               # first neuron in hidden layer\n",
    "abs_weights = map(abs, weights)       # darkness only depends on absolute value\n",
    "\n",
    "\n",
    "grid = [abs_weights[row:(row+5)]      # turn the weights into a 5x5 grid\n",
    "        for row in range(0,25,5)]     # [weights[0:5], ..., weights[20:25]]\n",
    "\n",
    "ax = plt.gca()                        # to use hatching, we'll need the axis\n",
    "\n",
    "ax.imshow(grid,                       # here same as plt.imshow\n",
    "          cmap=matplotlib.cm.binary,  # use white-black color scale\n",
    "          interpolation='none')       # plot blocks as blocks\n",
    "\n",
    "def patch(x, y, hatch, color):\n",
    "    \"\"\"return a matplotlib 'patch' object with the specified\n",
    "    location, crosshatch pattern, and color\"\"\"\n",
    "    return matplotlib.patches.Rectangle((x - 0.5, y - 0.5), 1, 1,\n",
    "                                        hatch=hatch, fill=False, color=color)\n",
    "\n",
    "# cross-hatch the negative weights\n",
    "for i in range(5):                    # row\n",
    "    for j in range(5):                # column\n",
    "        if weights[5*i + j] < 0:      # row i, column j = weights[5*i + j]\n",
    "            # add black and white hatches, so visible whether dark or light\n",
    "            ax.add_patch(patch(j, i, '/',  \"white\"))\n",
    "            ax.add_patch(patch(j, i, '\\\\', \"black\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999959524615\n",
      "0.954107627065\n",
      "8.71509188498e-09\n"
     ]
    }
   ],
   "source": [
    "left_column_only = [1, 0, 0, 0, 0] * 5\n",
    "print feed_forward(network, left_column_only)[0][0]  # 1.0\n",
    "\n",
    "center_middle_row = [0, 0, 0, 0, 0] * 2 + [0, 1, 1, 1, 0] + [0, 0, 0, 0, 0] * 2\n",
    "print feed_forward(network, center_middle_row)[0][0]  # 0.95\n",
    "\n",
    "right_column_only = [0, 0, 0, 0, 1] * 5\n",
    "print feed_forward(network, right_column_only)[0][0]  # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_three =  [0,1,1,1,0,  # .@@@.\n",
    "             0,0,0,1,1,  # ...@@\n",
    "             0,0,1,1,0,  # ..@@.\n",
    "             0,0,0,1,1,  # ...@@\n",
    "             0,1,1,1,0]  # .@@@.\n",
    "\n",
    "hidden, output = feed_forward(network, my_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9194716562747247"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(.121 * -11.61 + 1 * -2.17 + 1 * 9.31 - 1.38 * 1 - 0 * 11.47 - 1.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
