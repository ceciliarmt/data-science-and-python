{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdin and stdout\n",
    "# You can pipe data through them using sys.stdin and sys.stdout\n",
    "# For example, here is a script that reads in lines of text and spits back out the ones that match \n",
    "# a regular expression:\n",
    "\n",
    "# egrep.py\n",
    "import sys, re\n",
    "\n",
    "# sys.argv is the list of command-line arguments\n",
    "# sys.argv[0] is the name of the program itself\n",
    "# sys.argv[1] will be the regex specified at the command line\n",
    "regex = sys.argv[1]\n",
    "# for every line passed into the script\n",
    "for line in sys.stdin:\n",
    "    # if it matches the regex, write it to stdout\n",
    "    if re.search(regex, line):\n",
    "        sys.stdout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# This code counts the lines it receives and then writes out the count:\n",
    "\n",
    "# line_count.py\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "\n",
    "# print goes to sys.stdout\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here’s a script that counts the words in its input and writes out the most common ones:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "# most_common_words.py\n",
    "#import sys\n",
    "#from collections import Counter\n",
    "\n",
    "# pass in number of words as first argument\n",
    "#try:\n",
    "    #num_words = int(sys.argv[1])\n",
    "#except:\n",
    "    #print \"usage: most_common_words.py num_words\"\n",
    "    #sys.exit(1)   # non-zero exit code indicates error\n",
    "\n",
    "#counter = Counter(word.lower()  # lowercase words\n",
    "                  #for line in sys.stdin             #\n",
    "                  #for word in line.strip().split()  # split on spaces\n",
    "                  #if word)                          # skip empty 'words'\n",
    "\n",
    "#for word, count in counter.most_common(num_words):\n",
    "    #sys.stdout.write(str(count))\n",
    "    #sys.stdout.write(\"\\t\")\n",
    "    #sys.stdout.write(word)\n",
    "    #sys.stdout.write(\"\\n\")      \n",
    "\n",
    "#“after which you could do something like: \n",
    "#“C:\\DataScience>type the_bible.txt | python most_common_words.py 10\n",
    "#64193   the\n",
    "#51380   and\n",
    "#34753   of\n",
    "#13643   to\n",
    "#12799   that\n",
    "#12560   in\n",
    "#10263   he\n",
    "#9840    shall\n",
    "#8987    unto\n",
    "#8836    for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASICS OF TEXTFILES\n",
    "# The first step to working with a text file is to obtain a file object using open:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "# 'r' means read-only\n",
    "#file_for_reading = open('reading_file.txt', 'r')\n",
    "\n",
    "# 'w' is write -- will destroy the file if it already exists!\n",
    "#file_for_writing = open('writing_file.txt', 'w')\n",
    "\n",
    "# 'a' is append -- for adding to the end of the file\n",
    "#file_for_appending = open('appending_file.txt', 'a')\n",
    "\n",
    "# don't forget to close your files when you're done\n",
    "#file_for_writing.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Because it is easy to forget to close your files, you should always use them in a with block, \n",
    "# at the end of which they will be closed automatically:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#with open(filename,'r') as f:\n",
    "    #data = function_that_gets_data_from(f)\n",
    "\n",
    "# at this point f has already been closed, so don't try to use it\n",
    "#process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to read a whole text file, you can just iterate over the lines of the file using for:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#starts_with_hash = 0\n",
    "\n",
    "#with open('input.txt','r') as f:\n",
    "    #for line in file:               # look at each line in the file\n",
    "        #if re.match(\"^#\",line):     # use a regex to see if it starts with '#'\n",
    "            #starts_with_hash += 1   # if it does, add 1 to the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, imagine you have a file full of email addresses, one per line, \n",
    "# and that you need to generate a histogram of the domain\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#def get_domain(email_address):\n",
    "    #\"\"\"split on '@' and return the last piece\"\"\"\n",
    "    #return email_address.lower().split(\"@\")[-1]\n",
    "\n",
    "#with open('email_addresses.txt', 'r') as f:\n",
    "    #domain_counts = Counter(get_domain(line.strip())\n",
    "                            #for line in f\n",
    "                            #if \"@\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More frequently you’ll work with files with lots of data on each line. \n",
    "# These files are very often either comma-separated or tab-separated. \n",
    "# Each line has several fields, with a comma (or a tab) indicating where one field ends and the next field starts.\n",
    "\n",
    "# If your file has no headers (which means you probably want each row as a list, and which places the burden on you \n",
    "# to know what’s in each “column), you can use csv.reader to iterate over the rows, each of which will be \n",
    "# an appropriately split list.\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#import csv\n",
    "\n",
    "#with open('tab_delimited_stock_prices.txt', 'rb') as f:\n",
    "    #reader = csv.reader(f, delimiter='\\t')\n",
    "    #for row in reader:\n",
    "        #date = row[0]\n",
    "        #symbol = row[1]\n",
    "        #closing_price = float(row[2])\n",
    "        #process(date, symbol, closing_price)\n",
    "        \n",
    "# If your file has headers you can either skip the header row (with an initial call to reader.next()) or get each row as a dict\n",
    "# (with the headers as keys) by using csv.DictReader\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#with open('colon_delimited_stock_prices.txt', 'rb') as f:\n",
    "    #reader = csv.DictReader(f, delimiter=':')\n",
    "    #for row in reader:\n",
    "        #date = row[\"date\"]\n",
    "        #symbol = row[\"symbol\"]\n",
    "        #closing_price = float(row[\"closing_price\"])\n",
    "        #process(date, symbol, closing_price)\n",
    "        \n",
    "# You can similarly write out delimited data using csv.writer:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#today_prices = { 'AAPL' : 90.91, 'MSFT' : 41.68, 'FB' : 64.5 }\n",
    "\n",
    "#with open('comma_delimited_stock_prices.txt','wb') as f:\n",
    "    #writer = csv.writer(f, delimiter=',')\n",
    "    #for stock, price in today_prices.items():\n",
    "        #writer.writerow([stock, price])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML AND THE PARSING THEREOF\n",
    "\n",
    "# To get data out of HTML, we will use the BeautifulSoup library, which builds a tree out of the various elements\n",
    "# on a web page and provides a simple interface for accessing them.\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "#import requests\n",
    "#html = requests.get(\"http://www.example.com\").text\n",
    "#soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "\n",
    "# For example, to find the first <p> tag (and its contents) you can use:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "# first_paragraph = soup.find('p')        # or just soup.p”\n",
    "\n",
    "\n",
    "# You can get the text contents of a Tag using its text property:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "# first_paragraph_text = soup.p.text\n",
    "# first_paragraph_words = soup.p.text.split()\n",
    "\n",
    "\n",
    "# You can extract a tag’s attributes by treating it like a dict:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#first_paragraph_id = soup.p['id']       # raises KeyError if no 'id'\n",
    "#first_paragraph_id2 = soup.p.get('id')  # returns None if no 'id'\n",
    "\n",
    "\n",
    "# You can get multiple tags at once\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#all_paragraphs = soup.find_all('p')  # or just soup('p')\n",
    "#paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
    "\n",
    "\n",
    "#Frequently you’ll want to find tags with a specific class:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "#important_paragraphs = soup('p', {'class' : 'important'})\n",
    "#important_paragraphs2 = soup('p', 'important')\n",
    "#important_paragraphs3 = [p for p in soup('p')\n",
    "                         #if 'important' in p.get('class', [])]\n",
    "    \n",
    "\n",
    "    \n",
    "# If you want to find every <span> element that is contained inside a <div> element, you could do this:\n",
    "# - - - - - - - - - - - - -- - - - -- - - - - -- - - -- - - - - - - - - - - - - - - -- - - - - - - - - -- - - - - - \n",
    "\n",
    "# warning, will return the same span multiple times\n",
    "# if it sits inside multiple divs\n",
    "# be more clever if that's the case\n",
    "#spans_inside_divs = [span\n",
    "                     #for div in soup('div')     # for each <div> on the page\n",
    "                     #for span in div('span')]   # find each <span> inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: O’Reilly Books About Data\n",
    "\n",
    "# To figure out how to extract the data, let’s download one of those pages and feed it to Beautiful Soup”\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# you don't have to split the url like this unless it needs to fit in a book\n",
    "url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
    "\"data.do?sortby=publicationDate&page=1\"\n",
    "soup = BeautifulSoup(requests.get(url).text, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Each book (or video) seems to be uniquely contained in a <td> table cell element whose class is thumbtext\n",
    "\n",
    "# A good first step is to find all of the td thumbtext tag elements:\n",
    "tds = soup('td', 'thumbtext')\n",
    "print len(tds)\n",
    "# 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# “Next we’d like to filter out the videos. If we inspect the HTML further, we see that each td contains one or more\n",
    "# span elements whose class is pricelabel, and whose text looks like Ebook: or Video: or Print:. \n",
    "# It appears that the videos contain only one pricelabel, whose text starts with Video (after removing leading spaces) \n",
    "# This means we can test for videos with:\n",
    "\n",
    "def is_video(td):\n",
    "    \"\"\"it's a video if it has exactly one pricelabel, and if\n",
    "    the stripped text inside that pricelabel starts with 'Video'\"\"\"\n",
    "    pricelabels = td('span', 'pricelabel')\n",
    "    return (len(pricelabels) == 1 and\n",
    "            pricelabels[0].text.strip().startswith(\"Video\"))\n",
    "\n",
    "print len([td for td in tds if not is_video(td)])\n",
    "# 21 for me, might be different for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'td' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-852707856232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# the <a> tag inside the <div class=\"thumbheader\">:”\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thumbheader\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mauthor_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AuthorName'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mauthors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^By \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'td' is not defined"
     ]
    }
   ],
   "source": [
    "# “Now we’re ready to start pulling data out of the td elements. It looks like the book title is the text inside \n",
    "# the <a> tag inside the <div class=\"thumbheader\">:”\n",
    "\n",
    "title = td.find(\"div\", \"thumbheader\").a.text\n",
    "author_name = td.find('div', 'AuthorName').text\n",
    "authors = [x.strip() for x in re.sub(\"^By \", \"\", author_name).split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ISBN seems to be contained in the link that’s in the thumbheader <div>:\n",
    "\n",
    "#isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
    "\n",
    "# re.match captures the part of the regex in parentheses\n",
    "\n",
    "#isbn = re.match(\"/product/(.*)\\.do\", isbn_link).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the date is just the contents of the <span class=\"directorydate\">:\n",
    "\n",
    "#date = td.find(\"span\", \"directorydate\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_info(td):\n",
    "    \"\"\"given a BeautifulSoup <td> Tag representing a book,\n",
    "    extract the book's details and return a dict\"\"\"\n",
    "\n",
    "    title = td.find(\"div\", \"thumbheader\").a.text\n",
    "    by_author = td.find('div', 'AuthorName').text\n",
    "    authors = [x.strip() for x in re.sub(\"^By \", \"\", by_author).split(\",\")]\n",
    "    isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
    "    isbn = re.match(\"/product/(.*)\\.do\", isbn_link).groups()[0]\n",
    "    date = td.find(\"span\", \"directorydate\").text.strip()\n",
    "    \n",
    "    return {\n",
    "        \"title\" : title,\n",
    "        \"authors\" : authors,\n",
    "        \"isbn\" : isbn,\n",
    "        \"date\" : date\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "souping page 1 , 0  found so far\n",
      "souping page 2 , 0  found so far\n",
      "souping page 3 , 0  found so far\n",
      "souping page 4 , 0  found so far\n",
      "souping page 5 , 0  found so far\n",
      "souping page 6 , 0  found so far\n",
      "souping page 7 , 0  found so far\n",
      "souping page 8 , 0  found so far\n",
      "souping page 9 , 0  found so far\n",
      "souping page 10 , 0  found so far\n",
      "souping page 11 , 0  found so far\n",
      "souping page 12 , 0  found so far\n",
      "souping page 13 , 0  found so far\n",
      "souping page 14 , 0  found so far\n",
      "souping page 15 , 0  found so far\n",
      "souping page 16 , 0  found so far\n",
      "souping page 17 , 0  found so far\n",
      "souping page 18 , 0  found so far\n",
      "souping page 19 , 0  found so far\n",
      "souping page 20 , 0  found so far\n",
      "souping page 21 , 0  found so far\n",
      "souping page 22 , 0  found so far\n",
      "souping page 23 , 0  found so far\n",
      "souping page 24 , 0  found so far\n",
      "souping page 25 , 0  found so far\n",
      "souping page 26 , 0  found so far\n",
      "souping page 27 , 0  found so far\n",
      "souping page 28 , 0  found so far\n",
      "souping page 29 , 0  found so far\n",
      "souping page 30 , 0  found so far\n",
      "souping page 31 , 0  found so far\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "base_url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
    "           \"data.do?sortby=publicationDate&page=\"\n",
    "\n",
    "books = []\n",
    "\n",
    "NUM_PAGES = 31     # at the time of writing, probably more by now\n",
    "\n",
    "for page_num in range(1, NUM_PAGES + 1):\n",
    "    print \"souping page\", page_num, \",\", len(books), \" found so far\"\n",
    "    url = base_url + str(page_num)\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
    "\n",
    "    for td in soup('td', 'thumbtext'):\n",
    "        if not is_video(td):\n",
    "            books.append(book_info(td))\n",
    "\n",
    "    # respect the robots.txt!\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# “Now that we’ve collected the data, we can plot the number of books published each year”\n",
    "\n",
    "def get_year(book):\n",
    "    \"\"\"book[\"date\"] looks like 'November 2014' so we need to\n",
    "    split on the space and then take the second piece\"\"\"\n",
    "    return int(book[\"date\"].split()[1])\n",
    "\n",
    "# 2014 is the last complete year of data (when I ran this)\n",
    "from collections import Counter\n",
    "\n",
    "year_counts = Counter(get_year(book) for book in books\n",
    "                      if get_year(book) <= 2014)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "years = sorted(year_counts)\n",
    "book_counts = [year_counts[year] for year in years]\n",
    "plt.plot(years, book_counts)\n",
    "plt.ylabel(\"# of data books\")\n",
    "plt.title(\"Data is Big!\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'publicationYear': 2014, u'author': u'Joel Grus', u'topics': [u'data', u'science', u'data science'], u'title': u'Data Science Book'}\n"
     ]
    }
   ],
   "source": [
    "# JSON\n",
    "# Because HTTP is a protocol for transferring text, the data you request through a web API needs to be serialized \n",
    "# into a string format. Often this serialization uses JSON\n",
    "\n",
    "import json\n",
    "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                  \"author\" : \"Joel Grus\",\n",
    "                  \"publicationYear\" : 2014,\n",
    "                  \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "# parse the JSON to create a Python dict\n",
    "deserialized = json.loads(serialized)\n",
    "if \"data science\" in deserialized[\"topics\"]:\n",
    "    print deserialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most APIs these days require you to first authenticate yourself in order to use them”\n",
    "\n",
    "import requests, json\n",
    "endpoint = \"https://api.github.com/users/ceciliarmt/repos\"\n",
    "\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_5_repositories = sorted(repos,\n",
    "                             key=lambda r: r[\"created_at\"],\n",
    "                             reverse=True)[:5]\n",
    "\n",
    "last_5_languages = [repo[\"language\"]\n",
    "                    for repo in last_5_repositories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the Twitter APIs\n",
    "\n",
    "#from twython import Twython\n",
    "\n",
    "#twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "# search for tweets containing the phrase \"data science\"\n",
    "#for status in twitter.search(q='\"data science\"')[\"statuses\"]:\n",
    "    #user = status[\"user\"][\"screen_name\"].encode('utf-8')\n",
    "    #text = status[\"text\"].encode('utf-8')\n",
    "    #print user, \":\", text\n",
    "    #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from twython import TwythonStreamer\n",
    "\n",
    "# appending data to a global variable is pretty poor form\n",
    "# but it makes the example much simpler\n",
    "#tweets = []\n",
    "\n",
    "#class MyStreamer(TwythonStreamer):\n",
    "    #\"\"\"our own subclass of TwythonStreamer that specifies\n",
    "    #how to interact with the stream\"\"\"\n",
    "\n",
    "    #def on_success(self, data):\n",
    "        #\"\"\"\"what do we do when twitter sends us data? here data will be a Python dict representing a tweet\"\"\"\"\n",
    "        # only want to collect English-language tweets\n",
    "        #if data['lang'] == 'en':\n",
    "            #tweets.append(data)\n",
    "            #print \"received tweet #\", len(tweets)\n",
    "\n",
    "        # stop when we've collected enough\n",
    "        #if len(tweets) >= 1000:\n",
    "            #self.disconnect()\n",
    "\n",
    "    #def on_error(self, status_code, data):\n",
    "        #print status_code, data\n",
    "        #self.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# starts consuming public statuses that contain the keyword 'data'\n",
    "\n",
    "#stream.statuses.filter(track='data')\n",
    "\n",
    "# if instead we wanted to start consuming a sample of *all* public statuses\n",
    "# stream.statuses.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could find the most common hashtags with:\n",
    "\n",
    "#top_hashtags = Counter(hashtag['text'].lower()\n",
    "                       #for tweet in tweets\n",
    "                       #for hashtag in tweet[\"entities\"][\"hashtags\"])\n",
    "\n",
    "#print top_hashtags.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
