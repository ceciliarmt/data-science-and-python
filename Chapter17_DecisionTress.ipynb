{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2)\n",
    "               for p in class_probabilities\n",
    "               if p)                         # ignore zero probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count\n",
    "            for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\n",
    "    subsets is a list of lists of labeled data\"\"\"\n",
    "\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "\n",
    "    return sum( data_entropy(subset) * len(subset) / total_count\n",
    "                for subset in subsets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE TREE\n",
    "# The VP provides you with the interviewee data, consisting of (per your specification) pairs (input, label), \n",
    "# where each input is a dict of candidate attributes, and each label is either True (the candidate interviewed well)\n",
    "# or False (the candidate interviewed poorly). In particular, you are provided with each candidate’s level,\n",
    "# her preferred language, whether she is active on Twitter, and whether she has a PhD\n",
    "\n",
    "inputs = [\n",
    "    ({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'no'},    False),\n",
    "    ({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'yes'},   False),\n",
    "    ({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'no'},      True),\n",
    "    ({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'no'},   True),\n",
    "    ({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'no'},       True),\n",
    "    ({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'yes'},     False),\n",
    "    ({'level':'Mid', 'lang':'R', 'tweets':'yes', 'phd':'yes'},         True),\n",
    "    ({'level':'Senior', 'lang':'Python', 'tweets':'no', 'phd':'no'},  False),\n",
    "    ({'level':'Senior', 'lang':'R', 'tweets':'yes', 'phd':'no'},       True),\n",
    "    ({'level':'Junior', 'lang':'Python', 'tweets':'yes', 'phd':'no'},  True),\n",
    "    ({'level':'Senior', 'lang':'Python', 'tweets':'yes', 'phd':'yes'}, True),\n",
    "    ({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'yes'},     True),\n",
    "    ({'level':'Mid', 'lang':'Java', 'tweets':'yes', 'phd':'no'},       True),\n",
    "    ({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"each input is a pair (attribute_dict, label).\n",
    "    returns a dict : attribute_value -> inputs\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = input[0][attribute]   # get the value of the specified attribute\n",
    "        groups[key].append(input)   # then add this input to the correct list\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " level 0.0\n",
      "lang 0\n",
      "tweets 0\n",
      "phd 0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import math \n",
    "\n",
    "for key in ['level','lang','tweets','phd']:\n",
    "    print key, partition_entropy_by(inputs, key)\n",
    "\n",
    "# level 0.693536138896\n",
    "# lang 0.860131712855\n",
    "# tweets 0.788450457308\n",
    "# phd 0.892158928262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.0\n",
      "tweets 0.0\n",
      "phd 0\n"
     ]
    }
   ],
   "source": [
    "# For Senior candidates, we have a mix of Trues and Falses, so we need to split again:\n",
    "\n",
    "senior_inputs = [(input, label)\n",
    "                 for input, label in inputs if input[\"level\"] == \"Senior\"]\n",
    "\n",
    "for key in ['lang', 'tweets', 'phd']:\n",
    "    print key, partition_entropy_by(senior_inputs, key)\n",
    "\n",
    "# lang 0.4\n",
    "# tweets 0.0\n",
    "# phd 0.950977500433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('level',\n",
       " {'Junior': ('phd', {'no': True, 'yes': False}),\n",
       "  'Mid': True,\n",
       "  'Senior': ('tweets', {'no': False, 'yes': True})})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True represents a leaf node that returns True for any input, False represents a leaf node that returns False for\n",
    "# any input, and a tuple represents a decision node that, for any input, finds its attribute value, and classifies \n",
    "# the input using the corresponding subtree\n",
    "\n",
    "('level',\n",
    " {'Junior': ('phd', {'no': True, 'yes': False}),\n",
    "  'Mid': True,\n",
    "  'Senior': ('tweets', {'no': False, 'yes': True})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What should our hiring tree do if it encounters a candidate whose level is “Intern”? We’ll handle this case by \n",
    "# adding a None key that just predicts the most common label.\n",
    "\n",
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "\n",
    "    # if this is a leaf node, return its value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "\n",
    "    # otherwise this tree consists of an attribute to split on\n",
    "    # and a dictionary whose keys are values of that attribute\n",
    "    # and whose values of are subtrees to consider next\n",
    "    attribute, subtree_dict = tree\n",
    "\n",
    "    subtree_key = input.get(attribute)    # None if input is missing attribute\n",
    "\n",
    "    if subtree_key not in subtree_dict:   # if no subtree for key,\n",
    "        subtree_key = None                # we'll use the None subtree\n",
    "\n",
    "    subtree = subtree_dict[subtree_key]   # choose the appropriate subtree\n",
    "    return classify(subtree, input)       # and use it to classify the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All that’s left is to build the tree representation from our training data:\n",
    "\n",
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "\n",
    "    # if this is our first pass,\n",
    "    # all keys of the first input are split candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "\n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "\n",
    "    if num_trues == 0: return False     # no Trues? return a \"False\" leaf\n",
    "    if num_falses == 0: return True     # no Falses? return a \"True\" leaf\n",
    "\n",
    "    if not split_candidates:            # if no split candidates left\n",
    "        return num_trues >= num_falses  # return the majority leaf\n",
    "\n",
    "    # otherwise, split on the best attribute\n",
    "    best_attribute = min(split_candidates,\n",
    "                         key=partial(partition_entropy_by, inputs))\n",
    "\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates\n",
    "                      if a != best_attribute]\n",
    "\n",
    "    # recursively build the subtrees\n",
    "    subtrees = { attribute_value : build_tree_id3(subset, new_candidates)\n",
    "                 for attribute_value, subset in partitions.iteritems() }\n",
    "\n",
    "    subtrees[None] = num_trues > num_falses      # default case\n",
    "\n",
    "    return (best_attribute, subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the tree we built, every leaf consisted entirely of True inputs or entirely of False inputs.\n",
    "# This means that the tree predicts perfectly on the training data set. But we can also apply it to new data \n",
    "# that wasn’t in the training set:\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "tree = build_tree_id3(inputs)\n",
    "\n",
    "classify(tree, { \"level\" : \"Junior\",\n",
    "                 \"lang\" : \"Java\",\n",
    "                 \"tweets\" : \"yes\",\n",
    "                 \"phd\" : \"no\"} )        # True\n",
    "\n",
    "classify(tree, { \"level\" : \"Junior\",\n",
    "                 \"lang\" : \"Java\",\n",
    "                 \"tweets\" : \"yes\",\n",
    "                 \"phd\" : \"yes\"} )       # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And also to data with missing or unexpected values:\n",
    "\n",
    "classify(tree, { \"level\" : \"Intern\" } ) # True\n",
    "classify(tree, { \"level\" : \"Senior\" } ) # False”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST \n",
    "# Given how closely decision trees can fit themselves to their training data, it’s not surprising that they have a\n",
    "# tendency to overfit. One way of avoiding this is a technique called random forests, in which we build multiple\n",
    "# decision trees and let them vote on how to classify inputs:\n",
    "\n",
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " # if there's already few enough split candidates, look at all of them\n",
    "    \n",
    "    ##if len(split_candidates) <= self.num_split_candidates:\n",
    "        ##sampled_split_candidates = split_candidates\n",
    "    # otherwise pick a random sample\n",
    "    ##else:\n",
    "        ##sampled_split_candidates = random.sample(split_candidates, self.num_split_candidates)\n",
    "\n",
    "    # now choose the best attribute only from those candidates\n",
    "    ##best_attribute = min(sampled_split_candidates,\n",
    "        ##key=partial(partition_entropy_by, inputs))\n",
    "\n",
    "    ##partitions = partition_by(inputs, best_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
